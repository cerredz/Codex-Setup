Always start by creating or clearing the file .claude/reasoning/test-audit.md before analyzing any test suite. Your first and only task before proposing any test modifications, additions, or deletions is to conduct an exhaustive audit of the existing test coverage that systematically examines every critical failure point in the codebase and evaluates whether the current test suite adequately protects against system-breaking scenarios. You must first load and analyze the .claude/commands/tester.md file to understand our testing philosophy—tests must target logic breaks, state corruption, data integrity failures, security vulnerabilities, race conditions, cascade failures, and integration breakdowns, not trivial validations or superficial coverage metrics. Every test you analyze must be evaluated against the question: "Does this test protect against a scenario that could break core functionality, corrupt data, expose security flaws, or degrade system reliability in production?".

Begin your audit by mapping the complete system landscape: identify all backend routes with their authentication/authorization dependencies, core business logic functions handling state mutations or critical calculations, database operations including transactions and constraint enforcement, middleware configurations affecting request processing, and integration points where component failures could cascade. For each area of the codebase, document what tests currently exist (specify exact file paths in backend/tests/{feature}/test\_{scenario}.py format), what failure scenarios each test covers, and critically—what is NOT being tested. Cross-reference the existing test suite against the comprehensive context of routes, business logic, database schemas, and middleware to identify coverage gaps: routes without authentication bypass tests, database operations without constraint violation tests, business logic without boundary condition tests, state-changing operations without race condition tests, API endpoints without malformed input tests, error handlers without failure recovery tests, and integration points without cascade failure tests.

For each existing test file, perform quality assessment: categorize tests as either critical (protects against genuine production failure scenarios), marginal (provides some value but not essential), or trivial (tests superficial behavior with no meaningful protection). Flag tests that check trivial validations like data type matches, string formatting, or other assertions that don't prevent real failures—these are candidates for removal to improve signal-to-noise ratio in the test suite. Identify redundant tests where multiple test cases validate the same underlying failure scenario with superficial variation, and recommend consolidation. Evaluate test structure for maintainability issues: note tests with brittle assertions tied to implementation details rather than behavior contracts, tests missing proper fixture setup or teardown (especially database cleanup using the Junk database at backend/database/connect.py), tests with unclear failure messages, and tests lacking documentation about what critical scenario they prevent.

Document critical gaps in your audit report with severity ratings (critical/high/medium/low) based on production risk: specify exact untested scenarios like authentication bypass vectors (missing tests for expired tokens, tampered JWTs, privilege escalation, session hijacking), authorization failures (tests missing for cross-user resource access, role boundary violations, ownership checks), data corruption scenarios (concurrent writes without transaction isolation tests, constraint violations, orphaned records from cascade failures), integration breakdowns (upstream service failures, database connection exhaustion, message queue backlogs), error propagation failures (unhandled exceptions leaking sensitive data, improper error recovery, resource leaks), and performance degradation under load (N+1 query explosion, memory leaks, connection pool exhaustion). For each gap, reference the specific backend component (route, function, database operation, middleware) that lacks coverage, describe the precise failure scenario that could occur in production, explain the impact severity and blast radius, and note whether this scenario aligns with our testing philosophy from .claude/commands/tester.md.

Analyze test organization: verify tests are properly categorized into unit tests (isolated business logic and stateful operations), integration tests (component interactions and contract mismatches), end-to-end tests (complete workflows), and stress tests (performance under load)—flag any tests incorrectly categorized or missing categorization. Identify missing test infrastructure: note absence of shared fixtures for common setup patterns, missing mock implementations for external dependencies, lack of test data factories for complex domain objects, or inadequate database seeding strategies using the Junk database. Evaluate test execution characteristics: flag slow tests that should be optimized or moved to integration/e2e categories, tests with non-deterministic failures indicating race conditions or improper isolation, and tests dependent on external state or specific execution order.

For enhancement recommendations, prioritize based on production risk: list critical missing tests first (authentication/authorization failures, data corruption scenarios, cascade failures), then high-value integration tests, followed by edge case unit tests, and finally stress/performance tests. For each recommended test, specify the exact file path where it should be created, describe the precise failure scenario being tested and why it represents genuine system risk, identify which routes/functions/database operations are involved, note required fixtures or mocks, and provide implementation guidance referencing patterns from .claude/commands/tester.md. Include recommendations for test cleanup: specify trivial tests to remove (with justification for why they don't protect against meaningful failures), redundant tests to consolidate, and structural improvements to enhance maintainability.

Cross-reference your audit against common vulnerability patterns: verify tests exist for OWASP API Security Top 10 scenarios relevant to the codebase (broken authentication, authorization failures, injection attacks, security misconfiguration, data exposure), ensure database operations test for SQL injection via parameterized queries, confirm file operations test for path traversal, validate rate limiting and DoS protection have stress tests, and check that error handlers don't leak sensitive information in test assertions. Document compliance considerations if relevant (GDPR data handling, audit log integrity, encryption at rest/in transit) and ensure tests validate these requirements.

Your test-audit.md report must include: (1) Complete inventory of existing tests with quality ratings, (2) Critical gap analysis with severity-ranked missing test scenarios, (3) Redundant/trivial test identification with removal justification, (4) Structural improvement recommendations for maintainability, (5) Prioritized implementation roadmap organized by risk level. Do not write any new test code, modify existing tests, delete trivial tests, or restructure test organization until this test-audit.md report has been reviewed and explicitly approved—all test suite changes must be validated as necessary and correctly prioritized before any modifications begin.
